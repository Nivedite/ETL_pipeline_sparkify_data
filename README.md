# ETL_pipeline_sparkify_data
**Summary**
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. We 'll be able to test your database and ETL pipeline by running queries and do analysis.
**Database Schema**
**Fact Table**
**SONGPLAYS** - records in log data associated with song plays i.e. records with page NextSong songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
**Dimension Tables**
**USERS** - users in the app user_id, first_name, last_name, gender, level
**SONGS** - songs in music database song_id, title, artist_id, year, duration
**ARTISTS** - artists in music database artist_id, name, location, latitude, longitude
**TIME** - timestamps of records in songplays broken down into specific units start_time, hour, day, week, month, year, weekday
**Create tables and database in Postgres**
1.Connect to existing default database and Drops (if exists) and Creates the sparkify database. 
2.Establishes connection with the sparkify database and gets cursor to it. 
3.Create the tables needed (using sql queries)and close the connection.
**ETL Pipeline - EXTRACT**
1.The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 
2.The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
3.The file structure is walked, and relevant files are read in a pandas dataframes
**ETL Pipeline -TRANSFORM**
1.The song data is processed and loaded into the songs and artists dimension tables.
2.The log data is processed and loaded into the time and users dimension tables.
3.The songplays fact table is populated by querying the songs and artists tables to obtain the respective IDs based on song information.
4.The ETL process is performed for all files in the song and log datasets.
**ETL Pipeline - LOAD**
1.The formatted data is inserted into their destination tables
2.All the datas are stored in tables which are created in postgres

**Here are few question used for Business Questions**
1. Top 5 Songs which played most
Ans:
%sql SELECT COUNT(DISTINCT sp.songplay_id) AS "TotalPlays", s.title FROM songplays AS sp JOIN songs AS s ON sp.song_id = s.song_id GROUP BY s.title ORDER BY "TotalPlays" DESC LIMIT 5;

2. Gender Breakdown of Users
Ans:
%sql SELECT COUNT(user_id) AS "Total", gender FROM users GROUP BY gender ORDER BY "Total" DESC;

3. To find out which free users listen to music the most. This can be used by the marketing team to make special offers to convert them to the premium plan.
Ans: 
%sql SELECT s.user_id, Count(*) AS songplays_count FROM   songplays AS s JOIN users AS u ON s.user_id = u.user_id WHERE  u.level = 'free' GROUP  BY s.user_id ORDER  BY songplays_count DESC LIMIT  10;

4. Find area with highest amount of listening instances:

Ans: 
%sql SELECT location, count(location) as num_of_listeners FROM songplays group by location order by num_of_listeners desc limit 1;

5. Find amount of paying users:

Ans: %sql SELECT count(level) FROM users WHERE level = 'paid’;







  
 
















